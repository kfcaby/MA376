---
title: "RedSox2018"
author: "Kevin Cummiskey"
date: "November 14, 2019"
output: pdf_document
---

# Chapter 6.1 Comparing Proportions

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
redsox = read_csv(file = "Red Sox Season")
redsox = redsox %>% mutate(Result = factor(str_sub(`W/L`,start = 0, end = 1),
                                           levels = c("W","L")))
redsox = redsox %>% mutate(Field = case_when(is.na(X5) ~ "Home",
                                                   TRUE ~ "Away"))
```



\textbf{Question: Are the Red Sox better at Fenway Park?}

The data consists of results from 182 Red Sox games in the 2018 season.  Data for this activity is available at \url{https://www.baseball-reference.com/teams/BOS/2018-schedule-scores.shtml}


```{r, message = F, result = 'asis'}
head(redsox %>% select(`Gm#`,Tm,Opp,Result, Field))
summary = redsox %>% 
  group_by(Field)%>% 
  count(Result) %>% 
  spread(key = Field, value = n)
kable(summary, caption = "Results of the Red Sox 2018 Season")
```

## Measures of Association

Calculate the \textit{conditional proportions} of wins for home games and away games. (You will also hear conditional proportions referred to as \textit{chances, likelihood, risk}).

\vspace{1in}

Calculate the difference in conditional proportions (also called \textit{risk difference}) comparing home games to away games.

\vspace{1in}

Calculate the \textit{relative risk} for a win comparing home and away games. How does the risk difference and relative risk tell us something different?

\vspace{1in}

Calculate the \textit{odds} of winning at home and away. What are the smallest and largest values the odds can take? (see plot below)

\vspace{1in}

```{r, fig.height=2}
measures = data.frame(prob = seq(0,1, by = 0.05))
measures = measures %>% mutate(odds = prob/(1-prob))
measures %>% ggplot(aes(x = prob, y = odds)) + 
  geom_line() +
  geom_line(aes(y = prob))
```

Calculate the \textit{odds ratio} for wins comparing home and away games.  What are the smallest and largest values the odds ratio can take?  Let's say we take to log of the odds ratio - what are the smallest and largest values the \textit{log odds ratio} can take?

\vspace{1in}

## Inference on Difference in Proportions

What are the null and alternative hypotheses for this test?

\vspace{1in}

What is the statistic of interest for this test?

\vspace{1in}

### Theory-based test (two sample z-test)

```{r}
# two-sample z-test
phat_home = 57/81
phat_away = 51/81
phat = 108/162
#standardized statistic (pg 420)
z = (phat_home - phat_away)/sqrt(phat*(1-phat)*(1/81 + 1/81))
#p-value
2*(1-pnorm(z,0,1))
```

### Theory-based test ($\chi^2$ test)

Fill in the expected values in the table below if home/away has no effect and the Red Sox won 108 games. 

|Result | Away| Home| Total|
|:------|----:|----:|------|
|W      |     |     |   108|
|L      |     |     |    54|
|Total  |   81|   81|   182|

The $\chi^2$ test compares the observed counts in each cell to the expected counts.

\[ X^2 = \sum_{\text{all cells}} \frac{(\text{observed} - \text{expected})^2}{\text{expected}}\]

Calculate the $\chi^2$ statistic.

```{r}
#calculate overall win/loss percentage
summary_overall = redsox %>% group_by(Result) %>%
  count() %>% group_by() %>% mutate(perc = n/sum(n)) %>%
  select(-n)
#calculate expected wins
summary_homeaway = redsox %>% 
  group_by(Field,Result) %>%
  count() %>%
  left_join(summary_overall, by = "Result") %>%
  group_by(Field) %>%
  mutate(expected = perc*sum(n))
summary_homeaway  

#calculate chi-square statistic
chisq = summary_homeaway %>% group_by() %>%
  summarise(chisq = sum((n - expected)^2/expected))

1- pchisq(chisq$chisq, 1)
```

### Simulation-based test

```{r}
redsox.sim = redsox %>% select(Result, Field)
riskDiff.sim = c()
n.sims = 5000

for(i in 1:n.sims){
  summary.sim = redsox.sim %>% 
    mutate(Result.sim = sample(Result)) %>% #shuffle wins
    group_by(Field) %>%
    count(Result.sim) %>% mutate(p = n/sum(n)) #calculate win percentages
  riskDiff.sim[i] = summary.sim$p[3]-summary.sim$p[1]
}

hist(riskDiff.sim)
sum( abs(riskDiff.sim) > (phat_home - phat_away))/n.sims
```

What would we conclude from these tests?

\vspace{1in}

Is confounding an issue in this analysis?  What variables might we want to control for in order to reduce confounding?

\vspace{1in}

\newpage

## Intro to Logistic Regression

Let $Y_i$ be whether or not the Red Sox win game $i$ such that $Y_i \sim \text{Bernoulli}(\pi_i)$ be the probability the Red Sox win game $i$.

Here is our model:

\[\log\left(\frac{\pi_i}{1-\pi_i}\right) = \beta_0 + \beta_1 \text{Field}_i \]

where $\text{Field}_i$ is whether game $i$ was played on the home or away field.

How do we interpret $\beta_0$, $\beta_1$? Why is there no $\epsilon_i$ in this model?

\vspace{1in}

Let's fit the model.

```{r}
#reverse factor levels for result
#so win is 1 and loss is 0
redsox$Result = factor(redsox$Result,
                       levels = c("L","W"))
model_homeaway = glm(Result ~ Field, 
                     data = redsox, 
                     family = "binomial")
summary(model_homeaway)
```

Have we seen these estimates before?
